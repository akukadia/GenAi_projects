{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aakas\\anaconda3\\envs\\yt_summary\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\aakas\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aakas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aakas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up YouTube API client\n",
    "api_key = 'AIzaSyCtHg_opMWrE9gLyXgEatHNx1v9DqnGNAE'  # Replace with your YouTube Data API key\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\aakas\\anaconda3\\envs\\yt_summary\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aakas\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 4/4 [13:19<00:00, 199.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "#Check for CUDA availability\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # You might need to adjust this based on Kaggle's available models\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(video_id):\n",
    "    try:\n",
    "        request = youtube.videos().list(part=\"snippet\", id=video_id)\n",
    "        response = request.execute()\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            return response['items'][0]['snippet']\n",
    "        else:\n",
    "            print(f\"No video found with id: {video_id}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript_with_timestamps(video_id):\n",
    "    return YouTubeTranscriptApi.get_transcript(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds):\n",
    "    minutes, seconds = divmod(int(seconds), 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_segments(transcript, keywords):\n",
    "    text_segments = [entry['text'] for entry in transcript]\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_segments + [' '.join(keywords)])\n",
    "    \n",
    "    cosine_similarities = (tfidf_matrix * tfidf_matrix[-1].T).A.flatten()[:-1]\n",
    "    most_similar_indices = cosine_similarities.argsort()[-5:][::-1]  # Get top 5 relevant segments\n",
    "    \n",
    "    relevant_segments = [transcript[i] for i in most_similar_indices]\n",
    "    return relevant_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_length(text_length, min_ratio=0.1, max_ratio=0.3, min_length=256, max_length=2048):\n",
    "    \"\"\"Calculate dynamic lengths for input and new tokens based on the input text length.\"\"\"\n",
    "    ratio = max(min_ratio, min(max_ratio, 1000 / text_length))\n",
    "    dynamic_length = int(text_length * ratio)\n",
    "    input_length = max(min_length, min(dynamic_length, max_length))\n",
    "    new_tokens = max(64, min(input_length // 2, 512))  # Ensure at least 64 new tokens, at most 512\n",
    "    return input_length, new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bullet_points(transcript, keywords=None):\n",
    "    if keywords:\n",
    "        relevant_segments = get_relevant_segments(transcript, keywords)\n",
    "    else:\n",
    "        relevant_segments = transcript\n",
    "\n",
    "    full_text = \" \".join([segment['text'] for segment in relevant_segments])\n",
    "    text_length = len(full_text.split())\n",
    "    \n",
    "    # Calculate dynamic lengths\n",
    "    input_length, max_new_tokens = get_dynamic_length(text_length)\n",
    "    \n",
    "    prompt = f\"Summarize the following text in 5 bullet points:\\n\\n{full_text}\\n\\nBullet points:\"\n",
    "    \n",
    "    try:\n",
    "        generated_text = text_generator(prompt, max_new_tokens=max_new_tokens, num_return_sequences=1)[0]['generated_text']\n",
    "        bullet_points = generated_text.split(\"Bullet points:\")[-1].strip().split('\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in bullet point generation: {str(e)}\")\n",
    "        bullet_points = sent_tokenize(full_text)[:5]\n",
    "\n",
    "    # Assign timestamps to bullet points\n",
    "    timestamped_bullets = []\n",
    "    for i, bullet in enumerate(bullet_points):\n",
    "        if i < len(relevant_segments):\n",
    "            timestamp = format_timestamp(relevant_segments[i]['start'])\n",
    "        else:\n",
    "            timestamp = format_timestamp(relevant_segments[-1]['start'])\n",
    "        timestamped_bullets.append(f\"[{timestamp}] {bullet.strip('- ')}\")\n",
    "\n",
    "    return timestamped_bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_about_video(bullet_points, user_input, transcript_length):\n",
    "    context = \"Here are the main points of a video:\\n\" + \"\\n\".join(bullet_points)\n",
    "    prompt = f\"{context}\\n\\nHuman: {user_input}\\n\\nAssistant:\"\n",
    "    \n",
    "    # Calculate dynamic lengths for chat response\n",
    "    input_length, max_new_tokens = get_dynamic_length(transcript_length, min_ratio=0.2, max_ratio=0.5, min_length=512, max_length=4096)\n",
    "    \n",
    "    response = text_generator(prompt, max_new_tokens=max_new_tokens, num_return_sequences=1)[0]['generated_text']\n",
    "    return response.split(\"Assistant:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Ryan Reynolds and Hugh Jackman Go Claws Out While Eating Spicy Wings | Hot Ones\n",
      "Description: Ryan Reynolds and Hugh Jackman are the stars of Deadpool & Wolverine, perhaps 2024's most anticipate...\n",
      "Transcript length: 4439 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\aakas\\anaconda3\\envs\\yt_summary\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Video Summary (Bullet Points with Timestamps):\n",
      "[00:00:00] * Ryan Reynolds and Hugh Jackman join Sean Evans on Hot Ones for a fun and hilarious episode.\n",
      "[00:00:04] * They talk about their upcoming movie, Deadpool and Wolverine, and how it was a long time coming.\n",
      "[00:00:08] * Ryan shares a story about being a forklift driver at a Safeway store in Vancouver.\n",
      "[00:00:14] * Hugh talks about being a birthday clown and how he got booed by kids.\n",
      "[00:00:16] * They discuss the importance of character development in movies and how too much time and money can lead to a lack of focus on character.\n",
      "[00:00:17] * They play a game where they have to vote on which country has better food, Australia or Canada.\n",
      "[00:00:19] * They share their experiences with singing the national anthem and hosting awards shows.\n",
      "[00:00:21] * They talk about their friendship and how they appreciate each other's company.\n",
      "[00:00:22] * They tease their upcoming movie and encourage fans to check it out.\n",
      "[00:00:24] * The episode ends with a plug for Hot Ones season 24 and the 10-pack available on heatness.com.\n",
      "\n",
      "You can now chat about the video. Type 'quit' to exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: According to the transcript, Ryan Reynolds doesn't explicitly share his perspective on hot wings. However, it's likely that he participates in the spicy food challenge that is a staple of the Hot Ones show, where he has to eat increasingly spicy wings while being interviewed. If you're interested in knowing more about his experience with hot wings, you might want to watch the full episode of Hot Ones with Ryan Reynolds and Hugh Jackman!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Ryan Reynolds shared a story about being a forklift driver at a Safeway store in Vancouver. [00:00:08] in the video. Would you like to know more about it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Yes, I can help you with that! Can you please provide me with the main points of the video? I'll do my best to summarize them for you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Yes, the main points of the video are about Ryan Reynolds and Hugh Jackman joining Sean Evans on Hot Ones, their experiences, and their upcoming movie. Would you like me to summarize the video further or provide more information?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'm happy to help! However, I must correct you that Ryan Reynolds was never a forklift driver at a Safeway store in Vancouver. This is likely a joke or a fabrication. Ryan Reynolds is a Canadian actor known for his roles in movies like Deadpool, The Proposal, and Green Lantern. He has never publicly spoken about working as a forklift driver.\n",
      "\n",
      "It's possible that you're thinking of a different celebrity or a fictional story. If you have any more information or context about this claim, I'd be happy to help you investigate further!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    video_url = input(\"Enter the YouTube video URL: \")\n",
    "    video_id = video_url.split(\"v=\")[1]\n",
    "    \n",
    "    # Get video information\n",
    "    video_info = get_video_info(video_id)\n",
    "    if video_info:\n",
    "        print(f\"Title: {video_info['title']}\")\n",
    "        print(f\"Description: {video_info['description'][:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get transcript with timestamps\n",
    "        transcript = get_transcript_with_timestamps(video_id)\n",
    "        transcript_length = sum(len(entry['text'].split()) for entry in transcript)\n",
    "        print(f\"Transcript length: {transcript_length} words\")\n",
    "        \n",
    "        # Ask user for summary preference\n",
    "        summary_type = input(\"Do you want a full summary or a summary based on specific keywords? (full/keywords): \").lower()\n",
    "        \n",
    "        if summary_type == 'keywords':\n",
    "            keywords = input(\"Enter keywords (comma-separated): \").split(',')\n",
    "            keywords = [k.strip() for k in keywords]\n",
    "            bullet_points = generate_bullet_points(transcript, keywords)\n",
    "        else:\n",
    "            bullet_points = generate_bullet_points(transcript)\n",
    "        \n",
    "        print(\"\\nVideo Summary (Bullet Points with Timestamps):\")\n",
    "        for bullet in bullet_points:\n",
    "            print(bullet)\n",
    "        \n",
    "        # Chat loop\n",
    "        print(\"\\nYou can now chat about the video. Type 'quit' to exit.\")\n",
    "        while True:\n",
    "            user_input = input(\"\\nYou: \")\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            response = chat_about_video(bullet_points, user_input, transcript_length)\n",
    "            print(f\"Assistant: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        print(\"Unable to process the video. Please try another video or check your input.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_summary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
